{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset from Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"weuUhM15xraEmLMEAiPU\")\n",
    "project = rf.workspace(\"data-science-3-5kybl\").project(\"helmet-detection-lcyce\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"yolov7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyiqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import packages.image_evaluator as image_evaluator\n",
    "from packages.clahe import create_CLAHE\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import packages.lowlight_test as lowlight_test\n",
    "import packages.lowlight_train as lowlight_train\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "dataset = data.dataset('Helmet-Detection-2/train', 'Helmet-Detection-2/test', 'Helmet-Detection-2/valid')\n",
    "img_eval = image_evaluator.image_evaluator()\n",
    "all_images = {}\n",
    "for i, img_path in enumerate(dataset.train_images_path):\n",
    "    all_images[img_path] = dataset.train_img[i]\n",
    "\n",
    "for i, img_path in enumerate(dataset.test_images_path):\n",
    "    all_images[img_path] = dataset.test_img[i]\n",
    "\n",
    "for i, img_path in enumerate(dataset.valid_images_path):\n",
    "    all_images[img_path] = dataset.valid_img[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_img_paths = {}\n",
    "for img_path in all_images.keys():\n",
    "    save_img_paths[img_path] = {}\n",
    "    save_img_paths[img_path]['CLAHE'] = img_path.replace('dataset/', 'dataset/CLAHE_processed/')\n",
    "    save_img_paths[img_path]['ZDCE'] = img_path.replace('dataset/', 'dataset/ZDCE_processed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSaveError(Exception):\n",
    "   pass\n",
    "\n",
    "def save_image(image_path, image):\n",
    "    try:\n",
    "        image.save(image_path)\n",
    "    except Exception as e:\n",
    "        raise ImageSaveError(f\"Error saving image: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLAHE Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save CLAHE Images to dataset/CLAHE_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Images: 100%|██████████| 6633/6633 [01:46<00:00, 62.26image/s]\n"
     ]
    }
   ],
   "source": [
    "for img_path, img in tqdm(all_images.items(), desc=f\"Testing Images: \", unit=\"image\"):\n",
    "    img_zdce = create_CLAHE(img)\n",
    "    save_image(save_img_paths[img_path]['CLAHE'], img_zdce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Images for CLAHE: 100%|██████████| 279/279 [00:17<00:00, 15.85image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIQE : 3.0793857445491466\n",
      "PSNR : tensor([20.0368], device='cuda:0')\n",
      "NIQE : 3.0793857445491466\n",
      "PSNR : tensor([20.0368], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Scores for CLAHE\n",
    "test_scores = {}\n",
    "test_scores = {'niqe': [],\n",
    "                        'psnr': []}\n",
    "for img in tqdm(dataset.test_img, desc=f\"Saving Images for CLAHE: \", unit=\"image\"):\n",
    "    img_zdce = create_CLAHE(img)\n",
    "    img_bnw = img_eval.convert_to_grayscale(img)\n",
    "    niqe_score = img_eval.compute_niqe(img_zdce)\n",
    "    psnr_score = img_eval.compute_psnr(img_bnw, img_zdce)\n",
    "    test_scores['niqe'].append(niqe_score)\n",
    "    test_scores['psnr'].append(psnr_score)\n",
    "\n",
    "\n",
    "for time in test_scores.keys():\n",
    "    print(f\"NIQE : {sum(test_scores['niqe']) / len(test_scores['niqe'])}\")\n",
    "    print(f\"PSNR : {sum(test_scores['psnr']) / len(test_scores['psnr'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-DCE Train and Test (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 5805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive - Mapúa University\\Desktop Folder\\School Folder\\2023-2Term\\M2P1\\lowlight_train.py:78: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(DCE_net.parameters(),config.grad_clip_norm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 10 : 0.5947394371032715\n",
      "Loss at iteration 20 : 0.570685088634491\n",
      "Loss at iteration 30 : 0.521859884262085\n",
      "Loss at iteration 40 : 0.384112149477005\n",
      "Loss at iteration 50 : 0.6229775547981262\n",
      "Loss at iteration 60 : 0.3130388855934143\n",
      "Loss at iteration 70 : 0.4103785455226898\n",
      "Loss at iteration 80 : 0.258225679397583\n",
      "Loss at iteration 90 : 0.29635247588157654\n",
      "Loss at iteration 100 : 0.47787466645240784\n",
      "Loss at iteration 110 : 0.30879324674606323\n",
      "Loss at iteration 120 : 0.2851330041885376\n",
      "Loss at iteration 130 : 0.2806309759616852\n",
      "Loss at iteration 140 : 0.4583684802055359\n",
      "Loss at iteration 150 : 0.3029378652572632\n",
      "Loss at iteration 160 : 0.35152947902679443\n",
      "Loss at iteration 170 : 0.3611409664154053\n",
      "Loss at iteration 180 : 0.23215749859809875\n",
      "Loss at iteration 190 : 0.33556047081947327\n",
      "Loss at iteration 200 : 0.3035610318183899\n",
      "Loss at iteration 210 : 0.26950615644454956\n",
      "Loss at iteration 220 : 0.320589542388916\n",
      "Loss at iteration 230 : 0.3088638186454773\n",
      "Loss at iteration 240 : 0.2795245051383972\n",
      "Loss at iteration 250 : 0.2860858142375946\n",
      "Loss at iteration 260 : 0.329780250787735\n",
      "Loss at iteration 270 : 0.2964331805706024\n",
      "Loss at iteration 280 : 0.27787330746650696\n",
      "Loss at iteration 290 : 0.24142597615718842\n",
      "Loss at iteration 300 : 0.3652800917625427\n",
      "Loss at iteration 310 : 0.3399001359939575\n",
      "Loss at iteration 320 : 0.5317521691322327\n",
      "Loss at iteration 330 : 0.3066933751106262\n",
      "Loss at iteration 340 : 0.2718062400817871\n",
      "Loss at iteration 350 : 0.3110958933830261\n",
      "Loss at iteration 360 : 0.3243875801563263\n",
      "Loss at iteration 370 : 0.31404998898506165\n",
      "Loss at iteration 380 : 0.29166996479034424\n",
      "Loss at iteration 390 : 0.25905144214630127\n",
      "Loss at iteration 400 : 0.3155432939529419\n",
      "Loss at iteration 410 : 0.20405544340610504\n",
      "Loss at iteration 420 : 0.3696010708808899\n",
      "Loss at iteration 430 : 0.3382720351219177\n",
      "Loss at iteration 440 : 0.31815531849861145\n",
      "Loss at iteration 450 : 0.2057156264781952\n",
      "Loss at iteration 460 : 0.22921347618103027\n",
      "Loss at iteration 470 : 0.22770872712135315\n",
      "Loss at iteration 480 : 0.3287137448787689\n",
      "Loss at iteration 490 : 0.24381418526172638\n",
      "Loss at iteration 500 : 0.32876309752464294\n",
      "Loss at iteration 510 : 0.428651362657547\n",
      "Loss at iteration 520 : 0.3790964186191559\n",
      "Loss at iteration 530 : 0.2095148265361786\n",
      "Loss at iteration 540 : 0.23797470331192017\n",
      "Loss at iteration 550 : 0.2492310106754303\n",
      "Loss at iteration 560 : 0.2700774669647217\n",
      "Loss at iteration 570 : 0.21739095449447632\n",
      "Loss at iteration 580 : 0.251591295003891\n",
      "Loss at iteration 590 : 0.3199537992477417\n",
      "Loss at iteration 600 : 0.3704116940498352\n",
      "Loss at iteration 610 : 0.23762544989585876\n",
      "Loss at iteration 620 : 0.4362698793411255\n",
      "Loss at iteration 630 : 0.43083807826042175\n",
      "Loss at iteration 640 : 0.3241865038871765\n",
      "Loss at iteration 650 : 0.20045210421085358\n",
      "Loss at iteration 660 : 0.29902368783950806\n",
      "Loss at iteration 670 : 0.355190247297287\n",
      "Loss at iteration 680 : 0.4725259244441986\n",
      "Loss at iteration 690 : 0.3293421268463135\n",
      "Loss at iteration 700 : 0.3406616747379303\n",
      "Loss at iteration 710 : 0.22648459672927856\n",
      "Loss at iteration 720 : 0.2998819649219513\n",
      "Loss at iteration 10 : 0.37222060561180115\n",
      "Loss at iteration 20 : 0.20816877484321594\n",
      "Loss at iteration 30 : 0.3415283262729645\n",
      "Loss at iteration 40 : 0.3159696161746979\n",
      "Loss at iteration 50 : 0.27913376688957214\n",
      "Loss at iteration 60 : 0.28848159313201904\n",
      "Loss at iteration 70 : 0.41119223833084106\n",
      "Loss at iteration 80 : 0.30633318424224854\n",
      "Loss at iteration 90 : 0.3279212713241577\n",
      "Loss at iteration 100 : 0.3004853427410126\n",
      "Loss at iteration 110 : 0.30693918466567993\n",
      "Loss at iteration 120 : 0.2934425473213196\n",
      "Loss at iteration 130 : 0.2309286743402481\n",
      "Loss at iteration 140 : 0.36112868785858154\n",
      "Loss at iteration 150 : 0.26421985030174255\n",
      "Loss at iteration 160 : 0.3003675043582916\n",
      "Loss at iteration 170 : 0.23634058237075806\n",
      "Loss at iteration 180 : 0.26122134923934937\n",
      "Loss at iteration 190 : 0.26859140396118164\n",
      "Loss at iteration 200 : 0.2786480784416199\n",
      "Loss at iteration 210 : 0.1831234246492386\n",
      "Loss at iteration 220 : 0.35303354263305664\n",
      "Loss at iteration 230 : 0.40819716453552246\n",
      "Loss at iteration 240 : 0.18904642760753632\n",
      "Loss at iteration 250 : 0.18019525706768036\n",
      "Loss at iteration 260 : 0.2813601493835449\n",
      "Loss at iteration 270 : 0.2551257908344269\n",
      "Loss at iteration 280 : 0.1962350606918335\n",
      "Loss at iteration 290 : 0.194030299782753\n",
      "Loss at iteration 300 : 0.1848033368587494\n",
      "Loss at iteration 310 : 0.32308724522590637\n",
      "Loss at iteration 320 : 0.21423417329788208\n",
      "Loss at iteration 330 : 0.21358275413513184\n",
      "Loss at iteration 340 : 0.39857247471809387\n",
      "Loss at iteration 350 : 0.3273822069168091\n",
      "Loss at iteration 360 : 0.2645759880542755\n",
      "Loss at iteration 370 : 0.223234623670578\n",
      "Loss at iteration 380 : 0.19669298827648163\n",
      "Loss at iteration 390 : 0.21186207234859467\n",
      "Loss at iteration 400 : 0.2245333343744278\n",
      "Loss at iteration 410 : 0.33606383204460144\n",
      "Loss at iteration 420 : 0.2515488862991333\n",
      "Loss at iteration 430 : 0.32775282859802246\n",
      "Loss at iteration 440 : 0.34843161702156067\n",
      "Loss at iteration 450 : 0.1843586415052414\n",
      "Loss at iteration 460 : 0.2674456834793091\n",
      "Loss at iteration 470 : 0.20070624351501465\n",
      "Loss at iteration 480 : 0.27544909715652466\n",
      "Loss at iteration 490 : 0.24344059824943542\n",
      "Loss at iteration 500 : 0.19675004482269287\n",
      "Loss at iteration 510 : 0.3597140610218048\n",
      "Loss at iteration 520 : 0.23340028524398804\n",
      "Loss at iteration 530 : 0.3410166800022125\n",
      "Loss at iteration 540 : 0.2474183887243271\n",
      "Loss at iteration 550 : 0.34488049149513245\n",
      "Loss at iteration 560 : 0.27513039112091064\n",
      "Loss at iteration 570 : 0.23236030340194702\n",
      "Loss at iteration 580 : 0.23638468980789185\n",
      "Loss at iteration 590 : 0.3038385510444641\n",
      "Loss at iteration 600 : 0.2677080035209656\n",
      "Loss at iteration 610 : 0.20474231243133545\n",
      "Loss at iteration 620 : 0.25949257612228394\n",
      "Loss at iteration 630 : 0.33748552203178406\n",
      "Loss at iteration 640 : 0.35433292388916016\n",
      "Loss at iteration 650 : 0.31550419330596924\n",
      "Loss at iteration 660 : 0.30877307057380676\n",
      "Loss at iteration 670 : 0.2382020354270935\n",
      "Loss at iteration 680 : 0.19066163897514343\n",
      "Loss at iteration 690 : 0.2819487452507019\n",
      "Loss at iteration 700 : 0.18754489719867706\n",
      "Loss at iteration 710 : 0.27642467617988586\n",
      "Loss at iteration 720 : 0.3765976130962372\n",
      "Loss at iteration 10 : 0.2743151783943176\n",
      "Loss at iteration 20 : 0.26350995898246765\n",
      "Loss at iteration 30 : 0.1982022076845169\n",
      "Loss at iteration 40 : 0.2629615068435669\n",
      "Loss at iteration 50 : 0.20175471901893616\n",
      "Loss at iteration 60 : 0.2450173944234848\n",
      "Loss at iteration 70 : 0.28753113746643066\n",
      "Loss at iteration 80 : 0.26859259605407715\n",
      "Loss at iteration 90 : 0.22835594415664673\n",
      "Loss at iteration 100 : 0.20569607615470886\n",
      "Loss at iteration 110 : 0.31724992394447327\n",
      "Loss at iteration 120 : 0.1973714381456375\n",
      "Loss at iteration 130 : 0.2470773309469223\n",
      "Loss at iteration 140 : 0.4624955654144287\n",
      "Loss at iteration 150 : 0.3334730267524719\n",
      "Loss at iteration 160 : 0.278923898935318\n",
      "Loss at iteration 170 : 0.3258691430091858\n",
      "Loss at iteration 180 : 0.29381346702575684\n",
      "Loss at iteration 190 : 0.4071952700614929\n",
      "Loss at iteration 200 : 0.2856588661670685\n",
      "Loss at iteration 210 : 0.42284923791885376\n",
      "Loss at iteration 220 : 0.29394713044166565\n",
      "Loss at iteration 230 : 0.32591986656188965\n",
      "Loss at iteration 240 : 0.2594999074935913\n",
      "Loss at iteration 250 : 0.33190637826919556\n",
      "Loss at iteration 260 : 0.2434171885251999\n",
      "Loss at iteration 270 : 0.3606162667274475\n",
      "Loss at iteration 280 : 0.3435062766075134\n",
      "Loss at iteration 290 : 0.23969241976737976\n",
      "Loss at iteration 300 : 0.3286896049976349\n",
      "Loss at iteration 310 : 0.3014940023422241\n",
      "Loss at iteration 320 : 0.29167479276657104\n",
      "Loss at iteration 330 : 0.20372559130191803\n",
      "Loss at iteration 340 : 0.28962069749832153\n",
      "Loss at iteration 350 : 0.21232174336910248\n",
      "Loss at iteration 360 : 0.30178624391555786\n",
      "Loss at iteration 370 : 0.2527746558189392\n",
      "Loss at iteration 380 : 0.3575975298881531\n",
      "Loss at iteration 390 : 0.22341148555278778\n",
      "Loss at iteration 400 : 0.24501848220825195\n",
      "Loss at iteration 410 : 0.18411622941493988\n",
      "Loss at iteration 420 : 0.2966309189796448\n",
      "Loss at iteration 430 : 0.2233857959508896\n",
      "Loss at iteration 440 : 0.31187132000923157\n",
      "Loss at iteration 450 : 0.20878060162067413\n",
      "Loss at iteration 460 : 0.2509177029132843\n",
      "Loss at iteration 470 : 0.3101504147052765\n",
      "Loss at iteration 480 : 0.3226155638694763\n",
      "Loss at iteration 490 : 0.3043912649154663\n",
      "Loss at iteration 500 : 0.3235524296760559\n",
      "Loss at iteration 510 : 0.23909658193588257\n",
      "Loss at iteration 520 : 0.23671402037143707\n",
      "Loss at iteration 530 : 0.2460542768239975\n",
      "Loss at iteration 540 : 0.20304787158966064\n",
      "Loss at iteration 550 : 0.3012337386608124\n",
      "Loss at iteration 560 : 0.32035577297210693\n",
      "Loss at iteration 570 : 0.3470091223716736\n",
      "Loss at iteration 580 : 0.34178927540779114\n",
      "Loss at iteration 590 : 0.29021090269088745\n",
      "Loss at iteration 600 : 0.2193916290998459\n",
      "Loss at iteration 610 : 0.23885470628738403\n",
      "Loss at iteration 620 : 0.25459370017051697\n",
      "Loss at iteration 630 : 0.3803924024105072\n",
      "Loss at iteration 640 : 0.16036859154701233\n",
      "Loss at iteration 650 : 0.3997015357017517\n",
      "Loss at iteration 660 : 0.32865607738494873\n",
      "Loss at iteration 670 : 0.2199057936668396\n",
      "Loss at iteration 680 : 0.21158751845359802\n",
      "Loss at iteration 690 : 0.3109564483165741\n",
      "Loss at iteration 700 : 0.29156485199928284\n",
      "Loss at iteration 710 : 0.3019552528858185\n",
      "Loss at iteration 720 : 0.32412898540496826\n",
      "Loss at iteration 10 : 0.2850176990032196\n",
      "Loss at iteration 20 : 0.32352522015571594\n",
      "Loss at iteration 30 : 0.2554727792739868\n",
      "Loss at iteration 40 : 0.2617056369781494\n",
      "Loss at iteration 50 : 0.31582269072532654\n",
      "Loss at iteration 60 : 0.19698120653629303\n",
      "Loss at iteration 70 : 0.3551803529262543\n",
      "Loss at iteration 80 : 0.28940349817276\n",
      "Loss at iteration 90 : 0.1991061419248581\n",
      "Loss at iteration 100 : 0.29602083563804626\n",
      "Loss at iteration 110 : 0.24619904160499573\n",
      "Loss at iteration 120 : 0.2381838709115982\n",
      "Loss at iteration 130 : 0.22446110844612122\n",
      "Loss at iteration 140 : 0.3473004698753357\n",
      "Loss at iteration 150 : 0.19620837271213531\n",
      "Loss at iteration 160 : 0.2602519392967224\n",
      "Loss at iteration 170 : 0.2214777171611786\n",
      "Loss at iteration 180 : 0.26769039034843445\n",
      "Loss at iteration 190 : 0.4397379457950592\n",
      "Loss at iteration 200 : 0.20362237095832825\n",
      "Loss at iteration 210 : 0.235222727060318\n",
      "Loss at iteration 220 : 0.2049398124217987\n",
      "Loss at iteration 230 : 0.2463664710521698\n",
      "Loss at iteration 240 : 0.28745537996292114\n",
      "Loss at iteration 250 : 0.24338044226169586\n",
      "Loss at iteration 260 : 0.2000734508037567\n",
      "Loss at iteration 270 : 0.31817328929901123\n",
      "Loss at iteration 280 : 0.21094663441181183\n",
      "Loss at iteration 290 : 0.256910502910614\n",
      "Loss at iteration 300 : 0.2768831253051758\n",
      "Loss at iteration 310 : 0.2940254211425781\n",
      "Loss at iteration 320 : 0.2824786305427551\n",
      "Loss at iteration 330 : 0.3461287021636963\n",
      "Loss at iteration 340 : 0.18677788972854614\n",
      "Loss at iteration 350 : 0.45931750535964966\n",
      "Loss at iteration 360 : 0.2806764245033264\n",
      "Loss at iteration 370 : 0.19920329749584198\n",
      "Loss at iteration 380 : 0.20020249485969543\n",
      "Loss at iteration 390 : 0.20844754576683044\n",
      "Loss at iteration 400 : 0.18327578902244568\n",
      "Loss at iteration 410 : 0.3353870213031769\n",
      "Loss at iteration 420 : 0.2890445291996002\n",
      "Loss at iteration 430 : 0.2024986296892166\n",
      "Loss at iteration 440 : 0.42030206322669983\n",
      "Loss at iteration 450 : 0.2475903332233429\n",
      "Loss at iteration 460 : 0.34521424770355225\n",
      "Loss at iteration 470 : 0.244290292263031\n",
      "Loss at iteration 480 : 0.21329163014888763\n",
      "Loss at iteration 490 : 0.32667818665504456\n",
      "Loss at iteration 500 : 0.26493024826049805\n",
      "Loss at iteration 510 : 0.2955699861049652\n",
      "Loss at iteration 520 : 0.2555581033229828\n",
      "Loss at iteration 530 : 0.3253064751625061\n",
      "Loss at iteration 540 : 0.3747618496417999\n",
      "Loss at iteration 550 : 0.21762655675411224\n",
      "Loss at iteration 560 : 0.17468027770519257\n",
      "Loss at iteration 570 : 0.22788618505001068\n",
      "Loss at iteration 580 : 0.20800817012786865\n",
      "Loss at iteration 590 : 0.22804301977157593\n",
      "Loss at iteration 600 : 0.3272845149040222\n",
      "Loss at iteration 610 : 0.2983279228210449\n",
      "Loss at iteration 620 : 0.30458930134773254\n",
      "Loss at iteration 630 : 0.24362675845623016\n",
      "Loss at iteration 640 : 0.25649333000183105\n",
      "Loss at iteration 650 : 0.22795851528644562\n",
      "Loss at iteration 660 : 0.24665312469005585\n",
      "Loss at iteration 670 : 0.3921377658843994\n",
      "Loss at iteration 680 : 0.23694133758544922\n",
      "Loss at iteration 690 : 0.3832506537437439\n",
      "Loss at iteration 700 : 0.23641756176948547\n",
      "Loss at iteration 710 : 0.2443321943283081\n",
      "Loss at iteration 720 : 0.1805213838815689\n",
      "Loss at iteration 10 : 0.2515256106853485\n",
      "Loss at iteration 20 : 0.3118640184402466\n",
      "Loss at iteration 30 : 0.284201979637146\n",
      "Loss at iteration 40 : 0.27596044540405273\n",
      "Loss at iteration 50 : 0.3970201313495636\n",
      "Loss at iteration 60 : 0.258035272359848\n",
      "Loss at iteration 70 : 0.20380470156669617\n",
      "Loss at iteration 80 : 0.20790289342403412\n",
      "Loss at iteration 90 : 0.249207004904747\n",
      "Loss at iteration 100 : 0.29207876324653625\n",
      "Loss at iteration 110 : 0.24157793819904327\n",
      "Loss at iteration 120 : 0.32763397693634033\n",
      "Loss at iteration 130 : 0.17712968587875366\n",
      "Loss at iteration 140 : 0.23541447520256042\n",
      "Loss at iteration 150 : 0.2554498314857483\n",
      "Loss at iteration 160 : 0.28190866112709045\n",
      "Loss at iteration 170 : 0.2811044156551361\n",
      "Loss at iteration 180 : 0.26978206634521484\n",
      "Loss at iteration 190 : 0.21806594729423523\n",
      "Loss at iteration 200 : 0.3147397041320801\n",
      "Loss at iteration 210 : 0.34180158376693726\n",
      "Loss at iteration 220 : 0.266656756401062\n",
      "Loss at iteration 230 : 0.30582743883132935\n",
      "Loss at iteration 240 : 0.449075847864151\n",
      "Loss at iteration 250 : 0.2747008502483368\n",
      "Loss at iteration 260 : 0.18282245099544525\n",
      "Loss at iteration 270 : 0.33759409189224243\n",
      "Loss at iteration 280 : 0.21905025839805603\n",
      "Loss at iteration 290 : 0.2040470689535141\n",
      "Loss at iteration 300 : 0.4254416823387146\n",
      "Loss at iteration 310 : 0.2772544324398041\n",
      "Loss at iteration 320 : 0.26948320865631104\n",
      "Loss at iteration 330 : 0.3259769678115845\n",
      "Loss at iteration 340 : 0.3035335838794708\n",
      "Loss at iteration 350 : 0.24193263053894043\n",
      "Loss at iteration 360 : 0.2881014049053192\n",
      "Loss at iteration 370 : 0.40194907784461975\n",
      "Loss at iteration 380 : 0.27058154344558716\n",
      "Loss at iteration 390 : 0.2981625497341156\n",
      "Loss at iteration 400 : 0.20339474081993103\n",
      "Loss at iteration 410 : 0.343629390001297\n",
      "Loss at iteration 420 : 0.26299840211868286\n",
      "Loss at iteration 430 : 0.24920067191123962\n",
      "Loss at iteration 440 : 0.38429319858551025\n",
      "Loss at iteration 450 : 0.22194650769233704\n",
      "Loss at iteration 460 : 0.20317894220352173\n",
      "Loss at iteration 470 : 0.299977570772171\n",
      "Loss at iteration 480 : 0.3319728970527649\n",
      "Loss at iteration 490 : 0.2229883074760437\n",
      "Loss at iteration 500 : 0.3306516408920288\n",
      "Loss at iteration 510 : 0.25293269753456116\n",
      "Loss at iteration 520 : 0.20505130290985107\n",
      "Loss at iteration 530 : 0.24973326921463013\n",
      "Loss at iteration 540 : 0.2832539677619934\n",
      "Loss at iteration 550 : 0.2857508361339569\n",
      "Loss at iteration 560 : 0.26873236894607544\n",
      "Loss at iteration 570 : 0.1736929565668106\n",
      "Loss at iteration 580 : 0.3079248070716858\n",
      "Loss at iteration 590 : 0.31607744097709656\n",
      "Loss at iteration 600 : 0.2186872363090515\n",
      "Loss at iteration 610 : 0.2727796733379364\n",
      "Loss at iteration 620 : 0.2760445773601532\n",
      "Loss at iteration 630 : 0.30864056944847107\n",
      "Loss at iteration 640 : 0.20640981197357178\n",
      "Loss at iteration 650 : 0.25799065828323364\n",
      "Loss at iteration 660 : 0.338165819644928\n",
      "Loss at iteration 670 : 0.21715398132801056\n",
      "Loss at iteration 680 : 0.22672626376152039\n",
      "Loss at iteration 690 : 0.31773602962493896\n",
      "Loss at iteration 700 : 0.2397858202457428\n",
      "Loss at iteration 710 : 0.18111273646354675\n",
      "Loss at iteration 720 : 0.24303659796714783\n",
      "Loss at iteration 10 : 0.2721346914768219\n",
      "Loss at iteration 20 : 0.25446298718452454\n",
      "Loss at iteration 30 : 0.22454343736171722\n",
      "Loss at iteration 40 : 0.22952046990394592\n",
      "Loss at iteration 50 : 0.20178347826004028\n",
      "Loss at iteration 60 : 0.20105452835559845\n",
      "Loss at iteration 70 : 0.20700134336948395\n",
      "Loss at iteration 80 : 0.24611704051494598\n",
      "Loss at iteration 90 : 0.26782405376434326\n",
      "Loss at iteration 100 : 0.18397265672683716\n",
      "Loss at iteration 110 : 0.2980945408344269\n",
      "Loss at iteration 120 : 0.21340185403823853\n",
      "Loss at iteration 130 : 0.16426502168178558\n",
      "Loss at iteration 140 : 0.31196725368499756\n",
      "Loss at iteration 150 : 0.26561012864112854\n",
      "Loss at iteration 160 : 0.2536429762840271\n",
      "Loss at iteration 170 : 0.172968789935112\n",
      "Loss at iteration 180 : 0.24248312413692474\n",
      "Loss at iteration 190 : 0.24309170246124268\n",
      "Loss at iteration 200 : 0.21719926595687866\n",
      "Loss at iteration 210 : 0.33757585287094116\n",
      "Loss at iteration 220 : 0.18277892470359802\n",
      "Loss at iteration 230 : 0.1700807362794876\n",
      "Loss at iteration 240 : 0.23287856578826904\n",
      "Loss at iteration 250 : 0.26783305406570435\n",
      "Loss at iteration 260 : 0.30680862069129944\n",
      "Loss at iteration 270 : 0.23046280443668365\n",
      "Loss at iteration 280 : 0.3981912136077881\n",
      "Loss at iteration 290 : 0.24439114332199097\n",
      "Loss at iteration 300 : 0.2727474570274353\n",
      "Loss at iteration 310 : 0.36659491062164307\n",
      "Loss at iteration 320 : 0.2741660475730896\n",
      "Loss at iteration 330 : 0.2600124776363373\n",
      "Loss at iteration 340 : 0.1704249382019043\n",
      "Loss at iteration 350 : 0.2689628005027771\n",
      "Loss at iteration 360 : 0.2345835566520691\n",
      "Loss at iteration 370 : 0.3088825047016144\n",
      "Loss at iteration 380 : 0.2973872125148773\n",
      "Loss at iteration 390 : 0.399419367313385\n",
      "Loss at iteration 400 : 0.19337207078933716\n",
      "Loss at iteration 410 : 0.2741914391517639\n",
      "Loss at iteration 420 : 0.29177725315093994\n",
      "Loss at iteration 430 : 0.24593386054039001\n",
      "Loss at iteration 440 : 0.21268047392368317\n",
      "Loss at iteration 450 : 0.26104846596717834\n",
      "Loss at iteration 460 : 0.3501015901565552\n",
      "Loss at iteration 470 : 0.2688003480434418\n",
      "Loss at iteration 480 : 0.2137783169746399\n",
      "Loss at iteration 490 : 0.23550675809383392\n",
      "Loss at iteration 500 : 0.23440422117710114\n",
      "Loss at iteration 510 : 0.3558118939399719\n",
      "Loss at iteration 520 : 0.19417665898799896\n",
      "Loss at iteration 530 : 0.2291644960641861\n",
      "Loss at iteration 540 : 0.34638652205467224\n",
      "Loss at iteration 550 : 0.3234969973564148\n",
      "Loss at iteration 560 : 0.24230334162712097\n",
      "Loss at iteration 570 : 0.3209684193134308\n",
      "Loss at iteration 580 : 0.3444465398788452\n",
      "Loss at iteration 590 : 0.3010740578174591\n",
      "Loss at iteration 600 : 0.30035334825515747\n",
      "Loss at iteration 610 : 0.35048848390579224\n",
      "Loss at iteration 620 : 0.2459031343460083\n",
      "Loss at iteration 630 : 0.24429966509342194\n",
      "Loss at iteration 640 : 0.2614659070968628\n",
      "Loss at iteration 650 : 0.24655313789844513\n",
      "Loss at iteration 660 : 0.20375242829322815\n",
      "Loss at iteration 670 : 0.38947397470474243\n",
      "Loss at iteration 680 : 0.350562185049057\n",
      "Loss at iteration 690 : 0.2736576795578003\n",
      "Loss at iteration 700 : 0.27291804552078247\n",
      "Loss at iteration 710 : 0.20657730102539062\n",
      "Loss at iteration 720 : 0.1921883523464203\n",
      "Loss at iteration 10 : 0.24961230158805847\n",
      "Loss at iteration 20 : 0.3579149842262268\n",
      "Loss at iteration 30 : 0.3830859959125519\n",
      "Loss at iteration 40 : 0.2452750951051712\n",
      "Loss at iteration 50 : 0.3530810475349426\n",
      "Loss at iteration 60 : 0.20478011667728424\n",
      "Loss at iteration 70 : 0.32097071409225464\n",
      "Loss at iteration 80 : 0.3254724144935608\n",
      "Loss at iteration 90 : 0.24496062099933624\n",
      "Loss at iteration 100 : 0.28764596581459045\n",
      "Loss at iteration 110 : 0.22554385662078857\n",
      "Loss at iteration 120 : 0.2626059353351593\n",
      "Loss at iteration 130 : 0.21157622337341309\n",
      "Loss at iteration 140 : 0.25912439823150635\n",
      "Loss at iteration 150 : 0.2598060369491577\n",
      "Loss at iteration 160 : 0.3008773922920227\n",
      "Loss at iteration 170 : 0.35896360874176025\n",
      "Loss at iteration 180 : 0.30855682492256165\n",
      "Loss at iteration 190 : 0.27749526500701904\n",
      "Loss at iteration 200 : 0.20714133977890015\n",
      "Loss at iteration 210 : 0.3034670054912567\n",
      "Loss at iteration 220 : 0.4947672486305237\n",
      "Loss at iteration 230 : 0.2814949154853821\n",
      "Loss at iteration 240 : 0.28398945927619934\n",
      "Loss at iteration 250 : 0.2656726837158203\n",
      "Loss at iteration 260 : 0.2107638120651245\n",
      "Loss at iteration 270 : 0.23683208227157593\n",
      "Loss at iteration 280 : 0.23769785463809967\n",
      "Loss at iteration 290 : 0.2919614613056183\n",
      "Loss at iteration 300 : 0.34804725646972656\n",
      "Loss at iteration 310 : 0.24074919521808624\n",
      "Loss at iteration 320 : 0.2255067080259323\n",
      "Loss at iteration 330 : 0.29181885719299316\n",
      "Loss at iteration 340 : 0.23830708861351013\n",
      "Loss at iteration 350 : 0.2603569030761719\n",
      "Loss at iteration 360 : 0.4087875783443451\n",
      "Loss at iteration 370 : 0.2019485980272293\n",
      "Loss at iteration 380 : 0.2773728370666504\n",
      "Loss at iteration 390 : 0.23571789264678955\n",
      "Loss at iteration 400 : 0.21385101974010468\n",
      "Loss at iteration 410 : 0.2820746600627899\n",
      "Loss at iteration 420 : 0.26430436968803406\n",
      "Loss at iteration 430 : 0.2849109172821045\n",
      "Loss at iteration 440 : 0.19075995683670044\n",
      "Loss at iteration 450 : 0.17548424005508423\n",
      "Loss at iteration 460 : 0.33335205912590027\n",
      "Loss at iteration 470 : 0.28425344824790955\n",
      "Loss at iteration 480 : 0.23262569308280945\n",
      "Loss at iteration 490 : 0.21079593896865845\n",
      "Loss at iteration 500 : 0.2217634916305542\n",
      "Loss at iteration 510 : 0.28125807642936707\n",
      "Loss at iteration 520 : 0.26262155175209045\n",
      "Loss at iteration 530 : 0.2025778889656067\n",
      "Loss at iteration 540 : 0.29950544238090515\n",
      "Loss at iteration 550 : 0.26089203357696533\n",
      "Loss at iteration 560 : 0.3069400489330292\n",
      "Loss at iteration 570 : 0.26088470220565796\n",
      "Loss at iteration 580 : 0.31028103828430176\n",
      "Loss at iteration 590 : 0.20016644895076752\n",
      "Loss at iteration 600 : 0.3056340515613556\n",
      "Loss at iteration 610 : 0.2042568027973175\n",
      "Loss at iteration 620 : 0.3630627989768982\n",
      "Loss at iteration 630 : 0.17749284207820892\n",
      "Loss at iteration 640 : 0.23876161873340607\n",
      "Loss at iteration 650 : 0.3267979323863983\n",
      "Loss at iteration 660 : 0.2256099134683609\n",
      "Loss at iteration 670 : 0.25690436363220215\n",
      "Loss at iteration 680 : 0.21818336844444275\n",
      "Loss at iteration 690 : 0.3455699682235718\n",
      "Loss at iteration 700 : 0.2755301892757416\n",
      "Loss at iteration 710 : 0.19054338335990906\n",
      "Loss at iteration 720 : 0.2468509078025818\n",
      "Loss at iteration 10 : 0.2574256360530853\n",
      "Loss at iteration 20 : 0.20280277729034424\n",
      "Loss at iteration 30 : 0.2876904010772705\n",
      "Loss at iteration 40 : 0.370866060256958\n",
      "Loss at iteration 50 : 0.2481767237186432\n",
      "Loss at iteration 60 : 0.2989659011363983\n",
      "Loss at iteration 70 : 0.25516149401664734\n",
      "Loss at iteration 80 : 0.22599321603775024\n",
      "Loss at iteration 90 : 0.2544383704662323\n",
      "Loss at iteration 100 : 0.31473782658576965\n",
      "Loss at iteration 110 : 0.24562589824199677\n",
      "Loss at iteration 120 : 0.23035211861133575\n",
      "Loss at iteration 130 : 0.3184983730316162\n",
      "Loss at iteration 140 : 0.24491934478282928\n",
      "Loss at iteration 150 : 0.3731343150138855\n",
      "Loss at iteration 160 : 0.26854103803634644\n",
      "Loss at iteration 170 : 0.23648551106452942\n",
      "Loss at iteration 180 : 0.2779974639415741\n",
      "Loss at iteration 190 : 0.26100414991378784\n",
      "Loss at iteration 200 : 0.307124525308609\n",
      "Loss at iteration 210 : 0.284393310546875\n",
      "Loss at iteration 220 : 0.18208849430084229\n",
      "Loss at iteration 230 : 0.270359605550766\n",
      "Loss at iteration 240 : 0.1991528421640396\n",
      "Loss at iteration 250 : 0.18766510486602783\n",
      "Loss at iteration 260 : 0.2021687775850296\n",
      "Loss at iteration 270 : 0.30120688676834106\n",
      "Loss at iteration 280 : 0.36032336950302124\n",
      "Loss at iteration 290 : 0.4284746050834656\n",
      "Loss at iteration 300 : 0.380763977766037\n",
      "Loss at iteration 310 : 0.2487240731716156\n",
      "Loss at iteration 320 : 0.44087469577789307\n",
      "Loss at iteration 330 : 0.268024206161499\n",
      "Loss at iteration 340 : 0.3339756727218628\n",
      "Loss at iteration 350 : 0.2774469256401062\n",
      "Loss at iteration 360 : 0.4979715943336487\n",
      "Loss at iteration 370 : 0.25659653544425964\n",
      "Loss at iteration 380 : 0.28429269790649414\n",
      "Loss at iteration 390 : 0.2231009304523468\n",
      "Loss at iteration 400 : 0.27356263995170593\n",
      "Loss at iteration 410 : 0.28517845273017883\n",
      "Loss at iteration 420 : 0.2475813925266266\n",
      "Loss at iteration 430 : 0.20685693621635437\n",
      "Loss at iteration 440 : 0.28098443150520325\n",
      "Loss at iteration 450 : 0.20036044716835022\n",
      "Loss at iteration 460 : 0.15963904559612274\n",
      "Loss at iteration 470 : 0.21782737970352173\n",
      "Loss at iteration 480 : 0.19034478068351746\n",
      "Loss at iteration 490 : 0.20269078016281128\n",
      "Loss at iteration 500 : 0.31845518946647644\n",
      "Loss at iteration 510 : 0.21635860204696655\n",
      "Loss at iteration 520 : 0.3084419071674347\n",
      "Loss at iteration 530 : 0.25834232568740845\n",
      "Loss at iteration 540 : 0.199666827917099\n",
      "Loss at iteration 550 : 0.19706282019615173\n",
      "Loss at iteration 560 : 0.24433867633342743\n",
      "Loss at iteration 570 : 0.2622060775756836\n",
      "Loss at iteration 580 : 0.24877393245697021\n",
      "Loss at iteration 590 : 0.18810704350471497\n",
      "Loss at iteration 600 : 0.31395423412323\n",
      "Loss at iteration 610 : 0.20837785303592682\n",
      "Loss at iteration 620 : 0.23620247840881348\n",
      "Loss at iteration 630 : 0.24826952815055847\n",
      "Loss at iteration 640 : 0.18959105014801025\n",
      "Loss at iteration 650 : 0.33088406920433044\n",
      "Loss at iteration 660 : 0.4094051420688629\n",
      "Loss at iteration 670 : 0.22857466340065002\n",
      "Loss at iteration 680 : 0.4629952907562256\n",
      "Loss at iteration 690 : 0.21885108947753906\n",
      "Loss at iteration 700 : 0.2295624017715454\n",
      "Loss at iteration 710 : 0.2701224386692047\n",
      "Loss at iteration 720 : 0.2531548738479614\n",
      "Loss at iteration 10 : 0.20948494970798492\n",
      "Loss at iteration 20 : 0.2628488540649414\n",
      "Loss at iteration 30 : 0.3084840178489685\n",
      "Loss at iteration 40 : 0.31065499782562256\n",
      "Loss at iteration 50 : 0.2805836498737335\n",
      "Loss at iteration 60 : 0.2439030110836029\n",
      "Loss at iteration 70 : 0.28934445977211\n",
      "Loss at iteration 80 : 0.2327880561351776\n",
      "Loss at iteration 90 : 0.1846468448638916\n",
      "Loss at iteration 100 : 0.2332589030265808\n",
      "Loss at iteration 110 : 0.29544559121131897\n",
      "Loss at iteration 120 : 0.3173229694366455\n",
      "Loss at iteration 130 : 0.3204489052295685\n",
      "Loss at iteration 140 : 0.25738006830215454\n",
      "Loss at iteration 150 : 0.3295685648918152\n",
      "Loss at iteration 160 : 0.3600253462791443\n",
      "Loss at iteration 170 : 0.30105721950531006\n",
      "Loss at iteration 180 : 0.30465126037597656\n",
      "Loss at iteration 190 : 0.29480209946632385\n",
      "Loss at iteration 200 : 0.32896023988723755\n",
      "Loss at iteration 210 : 0.28173723816871643\n",
      "Loss at iteration 220 : 0.22190862894058228\n",
      "Loss at iteration 230 : 0.24552664160728455\n",
      "Loss at iteration 240 : 0.21330685913562775\n",
      "Loss at iteration 250 : 0.21877595782279968\n",
      "Loss at iteration 260 : 0.2611218988895416\n",
      "Loss at iteration 270 : 0.43807369470596313\n",
      "Loss at iteration 280 : 0.27048105001449585\n",
      "Loss at iteration 290 : 0.34553784132003784\n",
      "Loss at iteration 300 : 0.34606584906578064\n",
      "Loss at iteration 310 : 0.23951278626918793\n",
      "Loss at iteration 320 : 0.3476453125476837\n",
      "Loss at iteration 330 : 0.2648252546787262\n",
      "Loss at iteration 340 : 0.2502403259277344\n",
      "Loss at iteration 350 : 0.2067771852016449\n",
      "Loss at iteration 360 : 0.42877307534217834\n",
      "Loss at iteration 370 : 0.3876999318599701\n",
      "Loss at iteration 380 : 0.24344873428344727\n",
      "Loss at iteration 390 : 0.3333874046802521\n",
      "Loss at iteration 400 : 0.2493380606174469\n",
      "Loss at iteration 410 : 0.2862373888492584\n",
      "Loss at iteration 420 : 0.3863985240459442\n",
      "Loss at iteration 430 : 0.28027111291885376\n",
      "Loss at iteration 440 : 0.3611063063144684\n",
      "Loss at iteration 450 : 0.18145258724689484\n",
      "Loss at iteration 460 : 0.28528887033462524\n",
      "Loss at iteration 470 : 0.24912402033805847\n",
      "Loss at iteration 480 : 0.293628066778183\n",
      "Loss at iteration 490 : 0.26886555552482605\n",
      "Loss at iteration 500 : 0.26508691906929016\n",
      "Loss at iteration 510 : 0.17601290345191956\n",
      "Loss at iteration 520 : 0.3161585330963135\n",
      "Loss at iteration 530 : 0.1857801377773285\n",
      "Loss at iteration 540 : 0.23893901705741882\n",
      "Loss at iteration 550 : 0.333875447511673\n",
      "Loss at iteration 560 : 0.22292406857013702\n",
      "Loss at iteration 570 : 0.23350855708122253\n",
      "Loss at iteration 580 : 0.28319114446640015\n",
      "Loss at iteration 590 : 0.2854824364185333\n",
      "Loss at iteration 600 : 0.338167667388916\n",
      "Loss at iteration 610 : 0.2491636574268341\n",
      "Loss at iteration 620 : 0.4371040165424347\n",
      "Loss at iteration 630 : 0.23340588808059692\n",
      "Loss at iteration 640 : 0.357379674911499\n",
      "Loss at iteration 650 : 0.25102731585502625\n",
      "Loss at iteration 660 : 0.24984389543533325\n",
      "Loss at iteration 670 : 0.321081280708313\n",
      "Loss at iteration 680 : 0.26253974437713623\n",
      "Loss at iteration 690 : 0.31640827655792236\n",
      "Loss at iteration 700 : 0.21611079573631287\n",
      "Loss at iteration 710 : 0.18923091888427734\n",
      "Loss at iteration 720 : 0.2766890823841095\n",
      "Loss at iteration 10 : 0.3125320076942444\n",
      "Loss at iteration 20 : 0.20675456523895264\n",
      "Loss at iteration 30 : 0.19735173881053925\n",
      "Loss at iteration 40 : 0.30979567766189575\n",
      "Loss at iteration 50 : 0.22603607177734375\n",
      "Loss at iteration 60 : 0.29182541370391846\n",
      "Loss at iteration 70 : 0.18684783577919006\n",
      "Loss at iteration 80 : 0.26946935057640076\n",
      "Loss at iteration 90 : 0.18236392736434937\n",
      "Loss at iteration 100 : 0.277565062046051\n",
      "Loss at iteration 110 : 0.3023931384086609\n",
      "Loss at iteration 120 : 0.3111361563205719\n",
      "Loss at iteration 130 : 0.3045293688774109\n",
      "Loss at iteration 140 : 0.2182473987340927\n",
      "Loss at iteration 150 : 0.19223102927207947\n",
      "Loss at iteration 160 : 0.2655608057975769\n",
      "Loss at iteration 170 : 0.3795112073421478\n",
      "Loss at iteration 180 : 0.3099895417690277\n",
      "Loss at iteration 190 : 0.3013331890106201\n",
      "Loss at iteration 200 : 0.1744290143251419\n",
      "Loss at iteration 210 : 0.20036520063877106\n",
      "Loss at iteration 220 : 0.27054914832115173\n",
      "Loss at iteration 230 : 0.2624218165874481\n",
      "Loss at iteration 240 : 0.33559170365333557\n",
      "Loss at iteration 250 : 0.2975888252258301\n",
      "Loss at iteration 260 : 0.19614443182945251\n",
      "Loss at iteration 270 : 0.16032171249389648\n",
      "Loss at iteration 280 : 0.2597980797290802\n",
      "Loss at iteration 290 : 0.26825064420700073\n",
      "Loss at iteration 300 : 0.2695135474205017\n",
      "Loss at iteration 310 : 0.1663164496421814\n",
      "Loss at iteration 320 : 0.2123158872127533\n",
      "Loss at iteration 330 : 0.24359506368637085\n",
      "Loss at iteration 340 : 0.40370625257492065\n",
      "Loss at iteration 350 : 0.21740566194057465\n",
      "Loss at iteration 360 : 0.2404158115386963\n",
      "Loss at iteration 370 : 0.278572678565979\n",
      "Loss at iteration 380 : 0.35285770893096924\n",
      "Loss at iteration 390 : 0.23315969109535217\n",
      "Loss at iteration 400 : 0.3166216015815735\n",
      "Loss at iteration 410 : 0.25429946184158325\n",
      "Loss at iteration 420 : 0.24853235483169556\n",
      "Loss at iteration 430 : 0.2914266884326935\n",
      "Loss at iteration 440 : 0.2998083829879761\n",
      "Loss at iteration 450 : 0.18913844227790833\n",
      "Loss at iteration 460 : 0.2105576992034912\n",
      "Loss at iteration 470 : 0.2551177740097046\n",
      "Loss at iteration 480 : 0.32592982053756714\n",
      "Loss at iteration 490 : 0.21773484349250793\n",
      "Loss at iteration 500 : 0.28507333993911743\n",
      "Loss at iteration 510 : 0.3375089168548584\n",
      "Loss at iteration 520 : 0.20032651722431183\n",
      "Loss at iteration 530 : 0.20549990236759186\n",
      "Loss at iteration 540 : 0.3056350648403168\n",
      "Loss at iteration 550 : 0.25413304567337036\n",
      "Loss at iteration 560 : 0.1907944679260254\n",
      "Loss at iteration 570 : 0.3529640734195709\n",
      "Loss at iteration 580 : 0.362826943397522\n",
      "Loss at iteration 590 : 0.2317548394203186\n",
      "Loss at iteration 600 : 0.15556268393993378\n",
      "Loss at iteration 610 : 0.24037757515907288\n",
      "Loss at iteration 620 : 0.32418251037597656\n",
      "Loss at iteration 630 : 0.20758146047592163\n",
      "Loss at iteration 640 : 0.27671700716018677\n",
      "Loss at iteration 650 : 0.22931382060050964\n",
      "Loss at iteration 660 : 0.30349668860435486\n",
      "Loss at iteration 670 : 0.23853984475135803\n",
      "Loss at iteration 680 : 0.22201204299926758\n",
      "Loss at iteration 690 : 0.2172669768333435\n",
      "Loss at iteration 700 : 0.22347038984298706\n",
      "Loss at iteration 710 : 0.262954443693161\n",
      "Loss at iteration 720 : 0.23409564793109894\n"
     ]
    }
   ],
   "source": [
    "# Training of Zero-DCE\n",
    "args_dict = {\n",
    "    'lowlight_images_path': \"dataset/train/images/\",\n",
    "    'lr': 0.0001,\n",
    "    'weight_decay': 0.0001,\n",
    "    'grad_clip_norm': 0.1,\n",
    "    'num_epochs': 10,\n",
    "    'train_batch_size': 8,\n",
    "    'val_batch_size': 4,\n",
    "    'num_workers': 4,\n",
    "    'display_iter': 10,\n",
    "    'snapshot_iter': 10,\n",
    "    'snapshots_folder': \"snapshots/\",\n",
    "    'load_pretrain': False,\n",
    "    'pretrain_dir': \"snapshots/Epoch9.pth\"\n",
    "}\n",
    "\n",
    "config = argparse.Namespace(**args_dict)\n",
    "\n",
    "if not os.path.exists(config.snapshots_folder):\n",
    "    os.mkdir(config.snapshots_folder)\n",
    "\n",
    "lowlight_train.train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Images for DCE: 100%|██████████| 6633/6633 [04:16<00:00, 25.89image/s]\n"
     ]
    }
   ],
   "source": [
    "for img_path, img in tqdm(all_images.items(), desc=f\"Saving Images for DCE: \", unit=\"image\"):\n",
    "    img_zdce = lowlight_test.lowlight(img)\n",
    "    save_image(save_img_paths[img_path]['ZDCE'], img_zdce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIQE : 3.556564442303999\n",
      "PSNR : tensor([20.0368], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Testing for Zero-DCE\n",
    "test_results = {}\n",
    "\n",
    "test_images_path = glob.glob(f\"dataset/ZDCE_processed/test/images/*.jpg\")\n",
    "test_result_scores = {}\n",
    "test_result_scores = {'niqe': [],\n",
    "                        'psnr': []}\n",
    "\n",
    "for img_path in tqdm(test_images_path, desc=f\"Testing Images: \", unit=\"image\"):\n",
    "    img = Image.open(img_path)\n",
    "    img_zdce = lowlight_test.lowlight(img)\n",
    "    img_bnw = img_eval.convert_to_grayscale(img)\n",
    "    niqe_score = img_eval.compute_niqe(img_zdce)\n",
    "    psnr_score = img_eval.compute_psnr(img_bnw, img_zdce)\n",
    "    test_result_scores['niqe'].append(niqe_score)\n",
    "    test_result_scores['psnr'].append(psnr_score)\n",
    "\n",
    "print(f\"NIQE : {sum(test_result_scores['niqe']) / len(test_result_scores['niqe'])}\")\n",
    "print(f\"PSNR : {sum(test_scores['psnr']) / len(test_scores['psnr'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
